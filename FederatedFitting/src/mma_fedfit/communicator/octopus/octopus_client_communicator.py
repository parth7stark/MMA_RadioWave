import json
from omegaconf import OmegaConf, DictConfig
from proxystore.proxy import Proxy, extract
from typing import Union, Dict, OrderedDict, Tuple, Optional, Any
from mma_fedfit.agent import ClientAgent
from mma_fedfit.logger import ClientAgentFileLogger
import numpy as np
from .utils import serialize_tensor_to_base64, deserialize_tensor_from_base64
from diaspora_event_sdk import KafkaProducer, KafkaConsumer
import torch
import logging

class OctopusClientCommunicator:
    """
    Octopus communicator for federated learning clients.
    Contains functions to produce/consume/handle different events
    """
    def __init__(
        self, 
        client_agent: ClientAgent,
        client_id: Union[str, int],
        logger: Optional[ClientAgentFileLogger] = None,
    ):
        """        
        :param client_id: A unique client ID.
        :param max_message_size: The maximum message size in bytes.
        """
        self.client_id = client_id
        self.client_agent = client_agent
        self.logger = logger if logger is not None else self._default_logger()
        

        self.topic = self.client_agent.client_agent_config.comm_configs.octopus_configs.topic


        # Kafka producer for control messages and sending Embeddings
        self.producer = KafkaProducer()


        client_group_id = self.client_agent.client_agent_config.comm_configs.octopus_configs.group_id
        self.consumer = KafkaConsumer(
            self.topic,
            enable_auto_commit=True,
            auto_offset_reset="latest",  # This ensures it reads all past messages
            group_id=client_group_id
        )

    def on_server_started(self, data):
        """
        Called once the client sees the server is started and the config is published.
        We can parse the config or update local settings, then send 'ClientReady' event.
        """

        print(f"[Site {self.client_agent.get_id()}] Received ServerStarted event.", flush=True)
        self.logger.info(f"[Site {self.client_agent.get_id()}] Received ServerStarted event.")
        
        client_config = data["site_config"]  # This is the dict from OmegaConf
        self.client_agent.load_config(client_config)

        # Now publish "DetectorReady"
        # ready_msg = {
        #     "EventType": "SiteReady",
        #     "Site_id": self.client_id,
        #     "day_threshold": day_threshold,
        #     "has_data": has_data, #True or False
        #     "n_data_points": n_data_points,
        # }
        # self.producer.send(self.topic, ready_msg)
        # self.producer.flush()
        # print(f"[Site {self.client_id}] Published SiteReady event.")
        # self.logger.info(f"[Site {self.client_id}] Published SiteReady event.")

    def publish_site_ready(self, day_threshold, has_data, n_data_points):
        """
        Called once the client sees the server is started and the config is published.
        We can parse the config or update local settings, then send 'ClientReady' event.
        """

        # Now publish "DetectorReady"
        self.threshold = day_threshold
        ready_msg = {
            "EventType": "SiteReady",
            "Site_id": self.client_id,
            "day_threshold": self.threshold,
            "has_data": has_data, #True or False (boolean)
            "n_data_points": n_data_points,
        }
        self.producer.send(self.topic, ready_msg)
        self.producer.flush()
        print(f"[Site {self.client_id}] Published SiteReady event.")
        self.logger.info(f"[Site {self.client_id}] Published SiteReady event.")


    def send_local_results_Octopus(self, local_results, status, **kwargs):
        """
        Send local results to the server for global fitting via Octopus
        :param local_results: The local posterior samples generated by the client model.
        :param status: "DONE" if MCMC was run, "SKIPPED" if no data
        :param kwargs: Additional metadata to be sent to the server.
        :return: None for async and if sync communication return Metadata containing the server's acknowledgment status.
        """
        
        if '_client_id' in kwargs:
            client_id = str(kwargs["_client_id"])
            del kwargs["_client_id"]
        else:
            client_id = str(self.client_id)
        
        if status == "SKIPPED":
            # Don't serialize anything, just send empty placeholder
            data = {
                "EventType": "LocalMCMCDone",
                "site_id": client_id,
                "status": "SKIPPED",
                "chain": "none",   # or None or "null"
                "day_threshold": self.threshold,
            }
        else:
            # Keep local_chains as list
            local_chains = local_results["chain"].tolist() if isinstance(local_results["chain"], np.ndarray) else local_results["chain"]
            
            
            # Convert to tensor only for transfer
            if self.client_agent.use_proxystore:
                local_tensor = torch.tensor(local_chains)  # Convert list to tensor
                proxied_tensor = self.client_agent.proxystore.proxy(local_tensor)
                chains_b64 = serialize_tensor_to_base64(proxied_tensor)
            else:
                local_tensor = torch.tensor(local_chains)
                chains_b64 = serialize_tensor_to_base64(local_tensor)


            # print(f"[Site {client_id}] chains: {send_chains}", flush=True)
            # self.logger.info(f"[Site {client_id}] chains: {chains_b64}")

            # Build the JSON payload
            # Convert NumPy arrays to lists to avoid serialization issues
            data = {
                "EventType": "LocalMCMCDone",
                "site_id": client_id,
                "status": status,
                'chain': chains_b64,
                "day_threshold": self.threshold,
                # 'chain': local_results["chain"].tolist() if isinstance(local_results["chain"], np.ndarray) else local_results["chain"],
                # 'min_time': float(local_results["min_time"]) if isinstance(local_results["min_time"], np.generic) else local_results["min_time"],
                # 'max_time': float(local_results["max_time"]) if isinstance(local_results["max_time"], np.generic) else local_results["max_time"],
                # 'unique_frequencies': local_results["unique_frequencies"].tolist() if isinstance(local_results["unique_frequencies"], np.ndarray) else local_results["unique_frequencies"]
            }

    
        self.producer.send(
            self.topic,
            value=data
        )

        self.producer.flush()

        print(f"[Site {client_id}] Sent Local Posterior Samples", flush=True)
        self.logger.info(f"[Site {client_id}] Sent Local Posterior Samples")

        # return
    
    def get_best_estimate(self, data):
        """
        Print consensus parameter values with 68% confidence intervals.
    
        Assumes results_dict has keys:
        - 'median': median value
        - 'LL': 16th percentile (lower bound)
        - 'UL': 84th percentile (upper bound)
        
        results_dict: dict with structure:
        {
            "log(E0)": {"median": ..., "LL": ..., "UL": ...},
            ...
        }
        """
        results_dict = data["theta_est"]
        # global_min_time = data["global_min_time"]
        # global_max_time = data["global_max_time"]
        # unique_frequencies = data["unique_frequencies"]

        print("MCMC Best estimate of parameters", flush=True)
        self.logger.info("MCMC Best estimate of parameters")
        
        theta_est = []
        for param, stats in results_dict.items():
            median = stats["median"]
            LL = stats["LL"]
            UL = stats["UL"]
            err_minus = median - LL
            err_plus = UL - median
            
            theta_est.append(median)

            print(f"{param:>12s} = {median:.4f} +{err_plus:.4f} -{err_minus:.4f}")
            self.logger.info(f"{param:>12s} = {median:.4f} +{err_plus:.4f} -{err_minus:.4f}")
            

        return theta_est

    def handle_proposed_theta_message(self, data, local_data):
        

        client_id = str(self.client_id)

        iteration_no = data["iteration_no"]
        walker_no = data["walker_no"]
        theta = np.array(data["theta"])


        print(f"Site {client_id} received proposed theta. Iteration no: {iteration_no}, walker={walker_no}")
        self.logger.info(f"Site {client_id} received proposed theta. Iteration no: {iteration_no}, walker={walker_no}")

        
        log_likelihood = self.client_agent.generator.compute_local_log_likelihood(theta, local_data)
        
        # Build the JSON payload
        data = {
            "EventType": "LogLikelihoodComputed",
            "site_id": client_id,
            'local_likelihood': log_likelihood,
            "iteration_no": iteration_no,
            "walker_no": walker_no,
            "day_threshold": self.threshold,

        }
    
        self.producer.send(
            self.topic,
            value=data
        )

        self.producer.flush()

        print(f"[Site {client_id}] Sent Local log-likelihood. Iteration no: {iteration_no}, walker={walker_no}", flush=True)
        self.logger.info(f"[Site {client_id}] Sent Local log-likelihood. Iteration no: {iteration_no}, walker={walker_no}")

        return
    
    def _default_logger(self):
        """Create a default logger for the server if no logger provided."""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        fmt = logging.Formatter('[%(asctime)s %(levelname)-4s server]: %(message)s')
        s_handler = logging.StreamHandler()
        s_handler.setLevel(logging.INFO)
        s_handler.setFormatter(fmt)
        logger.addHandler(s_handler)
        return logger

