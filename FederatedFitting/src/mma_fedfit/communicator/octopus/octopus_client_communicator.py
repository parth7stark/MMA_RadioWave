import json
from omegaconf import OmegaConf, DictConfig
from proxystore.proxy import Proxy, extract
from typing import Union, Dict, OrderedDict, Tuple, Optional, Any
from mma_fedfit.agent import ClientAgent
from mma_fedfit.logger import ClientAgentFileLogger
import numpy as np
from .utils import serialize_tensor_to_base64, deserialize_tensor_from_base64
from diaspora_event_sdk import KafkaProducer, KafkaConsumer
import torch


class OctopusClientCommunicator:
    """
    Octopus communicator for federated learning clients.
    Contains functions to produce/consume/handle different events
    """
    def __init__(
        self, 
        client_agent: ClientAgent,
        client_id: Union[str, int],
        logger: Optional[ClientAgentFileLogger] = None,
    ):
        """        
        :param client_id: A unique client ID.
        :param max_message_size: The maximum message size in bytes.
        """
        self.client_id = client_id
        self.client_agent = client_agent
        self.logger = logger if logger is not None else self._default_logger()
        

        self.topic = self.client_agent.client_agent_config.comm_configs.octopus_configs.topic


        # Kafka producer for control messages and sending Embeddings
        self.producer = KafkaProducer()


        client_group_id = self.client_agent.client_agent_config.comm_configs.octopus_configs.group_id
        self.consumer = KafkaConsumer(
            self.topic,
            enable_auto_commit=True,
            auto_offset_reset="earliest",  # This ensures it reads all past messages
            group_id=client_group_id
        )

    def on_server_started(self, data):
        """
        Called once the client sees the server is started and the config is published.
        We can parse the config or update local settings, then send 'ClientReady' event.
        """

        print(f"[Site {self.client_agent.get_id()}] Received ServerStarted event.", flush=True)
        self.logger.info(f"[Site {self.client_agent.get_id()}] Received ServerStarted event.")
        
        client_config = data["site_config"]  # This is the dict from OmegaConf
        self.client_agent.load_config(client_config)

        # Now publish "DetectorReady"
        ready_msg = {
            "EventType": "SiteReady",
            "Site_id": self.client_id
        }
        self.producer.send(self.topic, ready_msg)
        self.producer.flush()
        print(f"[Site {self.client_id}] Published SiteReady event.")
        self.logger.info(f"[Site {self.client_id}] Published SiteReady event.")


    def send_local_results_Octopus(self, local_results, **kwargs):
        """
        Send local results to the server for global fitting via Octopus
        :param local_results: The local posterior samples generated by the client model.
        :param kwargs: Additional metadata to be sent to the server.
        :return: None for async and if sync communication return Metadata containing the server's acknowledgment status.
        """
        
        if '_client_id' in kwargs:
            client_id = str(kwargs["_client_id"])
            del kwargs["_client_id"]
        else:
            client_id = str(self.client_id)
        
        # Keep local_chains as list
        local_chains = local_results["chain"].tolist() if isinstance(local_results["chain"], np.ndarray) else local_results["chain"]
        
        
        # Convert to tensor only for transfer
        if self.client_agent.use_proxystore:
            local_tensor = torch.tensor(local_chains)  # Convert list to tensor
            proxied_tensor = self.client_agent.proxystore.proxy(local_tensor)
            chains_b64 = serialize_tensor_to_base64(proxied_tensor)
        else:
            local_tensor = torch.tensor(local_chains)
            chains_b64 = serialize_tensor_to_base64(local_tensor)


        # print(f"[Site {client_id}] chains: {send_chains}", flush=True)
        self.logger.info(f"[Site {client_id}] chains: {chains_b64}")

        # Build the JSON payload
        # Convert NumPy arrays to lists to avoid serialization issues
        data = {
            "EventType": "LocalMCMCDone",
            "site_id": client_id,
            "status": "DONE",
            'chain': chains_b64,
            # 'chain': local_results["chain"].tolist() if isinstance(local_results["chain"], np.ndarray) else local_results["chain"],
            'min_time': float(local_results["min_time"]) if isinstance(local_results["min_time"], np.generic) else local_results["min_time"],
            'max_time': float(local_results["max_time"]) if isinstance(local_results["max_time"], np.generic) else local_results["max_time"],
            'unique_frequencies': local_results["unique_frequencies"].tolist() if isinstance(local_results["unique_frequencies"], np.ndarray) else local_results["unique_frequencies"]
        }

    
        self.producer.send(
            self.topic,
            value=data
        )

        self.producer.flush()

        print(f"[Site {client_id}] Sent Local Posterior Samples", flush=True)
        self.logger.info(f"[Site {client_id}] Sent Local Posterior Samples")

        return
    
    def get_best_estimate(self, data):
        theta_est = data["theta_est"]
        global_min_time = data["global_min_time"]
        global_max_time = data["global_max_time"]
        unique_frequencies = data["unique_frequencies"]

        print("Best estimate of parameters", flush=True)
        self.logger.info("Best estimate of parameters")
        params = ['log(E0)','thetaObs','thetaCore','log(n0)',
                  'log(epsilon_e)','log(epsilon_B)','p']
        
        ndim = 7
        for i in range(ndim):
            if i in [0, 3, 4, 5]:  # Reverse the transformation for log-space parameters
                log_value = np.log10(theta_est[i])  # Convert back to log-space
                print(f'{params[i]} = {log_value:.2f}', flush=True)
                self.logger.info(f'{params[i]} = {log_value:.2f}')
            else:
                print(f'{params[i]} = {theta_est[i]:.2f}', flush=True)
                self.logger.info(f'{params[i]} = {theta_est[i]:.2f}')

        return theta_est, global_min_time, global_max_time, unique_frequencies

    def handle_proposed_theta_message(self, data, local_data):
        
        if '_client_id' in kwargs:
            client_id = str(kwargs["_client_id"])
            del kwargs["_client_id"]
        else:
            client_id = str(self.client_id)

        iteration_no = data["iteration_no"]
        theta = data["theta"]


        print(f"Site {client_id} received proposed theta.")
        self.logger.info(f"Site {client_id} received proposed theta.")

        
        log_likelihood = client_agent.compute_log_likelihood(theta, local_data)
        
        # Build the JSON payload
        data = {
            "EventType": "LogLikelihoodComputed",
            "site_id": client_id,
            'local_likelihood': log_likelihood,
        }
    
        self.producer.send(
            self.topic,
            value=data
        )

        self.producer.flush()

        print(f"[Site {client_id}] Sent Local log-likelihood", flush=True)
        self.logger.info(f"[Site {client_id}] Sent Local log-likelihood")

        return
    
    def _default_logger(self):
        """Create a default logger for the server if no logger provided."""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        fmt = logging.Formatter('[%(asctime)s %(levelname)-4s server]: %(message)s')
        s_handler = logging.StreamHandler()
        s_handler.setLevel(logging.INFO)
        s_handler.setFormatter(fmt)
        logger.addHandler(s_handler)
        return logger

