import json
from omegaconf import OmegaConf, DictConfig
from proxystore.proxy import Proxy, extract
from typing import Union, Dict, OrderedDict, Tuple, Optional, Any
from mma_gw.agent import ClientAgent
from .utils import serialize_tensor_to_base64, deserialize_tensor_from_base64
from mma_gw.logger import ClientAgentFileLogger

from diaspora_event_sdk import KafkaProducer, KafkaConsumer


class OctopusClientCommunicator:
    """
    Octopus communicator for federated learning clients.
    Contains functions to produce/consume/handle different events
    """
    def __init__(
        self, 
        client_agent: ClientAgent,
        client_id: Union[str, int],
        logger: Optional[ClientAgentFileLogger] = None,
    ):
        """        
        :param client_id: A unique client ID.
        :param max_message_size: The maximum message size in bytes.
        """
        self.client_id = client_id
        self.client_agent = client_agent
        self.logger = logger if logger is not None else self._default_logger()
        

        self.topic = self.client_agent.client_agent_config.comm_configs.octopus_configs.topic


        # Kafka producer for control messages and sending Embeddings
        self.producer = KafkaProducer()


        client_group_id = self.client_agent.client_agent_config.comm_configs.octopus_configs.group_id
        self.consumer = KafkaConsumer(
            self.topic,
            enable_auto_commit=True,
            auto_offset_reset="earliest",  # This ensures it reads all past messages
            group_id=client_group_id
        )

    def on_server_started(self, data):
        """
        Called once the client sees the server is started and the config is published.
        We can parse the config or update local settings, then send 'ClientReady' event.
        """

        print(f"[Site {self.client_agent.get_id()}] Received ServerStarted event.", flush=True)
        self.logger.info(f"[Site {self.client_agent.get_id()}] Received ServerStarted event.")
        
        client_config = data["site_config"]  # This is the dict from OmegaConf
        self.client_agent.load_config(client_config)

        # Now publish "DetectorReady"
        ready_msg = {
            "EventType": "SiteReady",
            "Site_id": self.client_id
        }
        self.producer.send(self.topic, ready_msg)
        self.producer.flush()
        print(f"[Site {self.client_id}] Published SiteReady event.")
        self.logger.info(f"[Site {self.client_id}] Published SiteReady event.")


    def send_local_results_Octopus(self, local_results, **kwargs):
        """
        Send local results to the server for global fitting via Octopus
        :param local_results: The local posterior samples generated by the client model.
        :param kwargs: Additional metadata to be sent to the server.
        :return: None for async and if sync communication return Metadata containing the server's acknowledgment status.
        """
        
        if '_client_id' in kwargs:
            client_id = str(kwargs["_client_id"])
            del kwargs["_client_id"]
        else:
            client_id = str(self.client_id)
        

        # Build the JSON payload
        data = {
            "EventType": "LocalMCMCDone",
            "site_id": client_id,
            'chain': local_results["chain"],
            'min_time': local_results["min_time"],
            'max_time': local_results["max_time"],
            'unique_frequencies': local_results["unique_frequencies"]
        }
    
        self.producer.send(
            self.topic,
            value=data
        )

        self.producer.flush()

        print(f"[Site {client_id}] Sent Local Posterior Samples", flush=True)
        self.logger.info(f"[Site {client_id}] Sent Local Posterior Samples")

        return
    
    def get_best_estimate(self, data):
        theta_est = data["theta_est"]
        global_min_time = data["global_min_time"]
        global_max_time = data["global_max_time"]
        unique_frequencies = data["unique_frequencies"]

        print("Best estimate of parameters", flush=True)
        self.logger.info("Best estimate of parameters")
        params = ['log(E0)','thetaObs','thetaCore','log(n0)',
                  'log(epsilon_e)','log(epsilon_B)','p']
        
        ndim = 7
        for i in range(ndim):
            print(f'{params[i]} = {theta_est[i]:.2f}', flush=True)
            self.logger.info(f'{params[i]} = {theta_est[i]:.2f}')

        return theta_est, global_min_time, global_max_time, unique_frequencies

    def handle_proposed_theta_message(self, data, local_data):
        
        if '_client_id' in kwargs:
            client_id = str(kwargs["_client_id"])
            del kwargs["_client_id"]
        else:
            client_id = str(self.client_id)

        iteration_no = data["iteration_no"]
        theta = data["theta"]


        print(f"Site {client_id} received proposed theta.")
        self.logger.info(f"Site {client_id} received proposed theta.")

        
        log_likelihood = client_agent.compute_log_likelihood(theta, local_data)
        
        # Build the JSON payload
        data = {
            "EventType": "LogLikelihoodComputed",
            "site_id": client_id,
            'local_likelihood': log_likelihood,
        }
    
        self.producer.send(
            self.topic,
            value=data
        )

        self.producer.flush()

        print(f"[Site {client_id}] Sent Local log-likelihood", flush=True)
        self.logger.info(f"[Site {client_id}] Sent Local log-likelihood")

        return


    def invoke_post_process(self, GPSStartTime):
        """
        Publish PostProcess event to the server for invoking post process pipeline via Octopus
        :param GPS start time: Starting GPS time for the inference data
        :return: None for async and if sync communication return Metadata containing the server's acknowledgment status.
        """
        
        detector_id =  self.client_id
        done_msg = {
            "EventType": "PostProcess",
            "detector_id": detector_id,
            "status" : "DONE",
            "details": "All Embeddings sent for given time segment -> Invoke post process pipeline",
            "GPS_start_time": GPSStartTime
        }


        self.producer.send(
            self.topic,
            value=done_msg
        )
        
        print(f"[Detector {detector_id}] Publish PostProcess Event: GPS start time={GPSStartTime}", flush=True)
        self.logger.info(f"[Detector {detector_id}] Publish PostProcess Event: GPS start time={GPSStartTime}")

        self.producer.flush()
        
        return

    
    def _default_logger(self):
        """Create a default logger for the server if no logger provided."""
        logger = logging.getLogger(__name__)
        logger.setLevel(logging.INFO)
        fmt = logging.Formatter('[%(asctime)s %(levelname)-4s server]: %(message)s')
        s_handler = logging.StreamHandler()
        s_handler.setLevel(logging.INFO)
        s_handler.setFormatter(fmt)
        logger.addHandler(s_handler)
        return logger

